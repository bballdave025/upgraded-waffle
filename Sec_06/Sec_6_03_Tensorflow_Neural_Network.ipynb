{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Set Random Seeds for same results</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(101)\n",
    "tf.set_random_seed(101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The graph we will be building\n",
    "\n",
    "Getting a very simple linear fit to some 2d data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<div>\n",
    "  <img src=\"./graph_for_Jose_06-29.jpg\"\n",
    "       alt=\"The TensorFlow Graph which we will be building for section 6-29.\"\n",
    "       width=\"500px\">\n",
    "</div>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Data Setup</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting Up some Random Data for Demonstration Purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_a = np.random.uniform(0, 100, (5, 5))\n",
    "rand_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_b = np.random.uniform(0, 100, (5, 1))\n",
    "rand_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tf.placeholder(tf.float32) # we're not using the shape parameter for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_op = a + b  # Could do tf.add(a, b), and [Shift]+[Tab] to look at the docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mult_op = a * b # Could do tf.multiply(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Sessions to create Graphs with Feed Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    add_result = sess.run(add_op, feed_dict={a:10, b:20})\n",
    "    print(add_result)\n",
    "    \n",
    "##endof:  with tf.Session() as sess:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    add_result = sess.run(add_op, feed_dict={a:rand_a, b:rand_b})\n",
    "    print(add_result)\n",
    "    \n",
    "    print('\\n')\n",
    "    \n",
    "    mult_result = sess.run(mult_op, feed_dict={a:rand_a, b:rand_b})\n",
    "    print(mult_result)\n",
    "    \n",
    "##endof:  with tf.Session as sess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplication was element-by-element rather than a `dot` multiply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 10\n",
    "n_dense_neurons = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for x\n",
    "x = tf.placeholder(tf.float32,) #  shape is defined, though the 'None' is\n",
    "                                #+ because it depends on how big of a \n",
    "                                #+ batch of data you're feeding into your NN\n",
    "                                #+ feautures. We should be expecting 'x'\n",
    "                                #+ to be receiving an array with n_samples\n",
    "                                #+ by n_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_normal([n_features, n_dense_neurons]))\n",
    "\n",
    "b = tf.Variable(tf.ones([n_dense_neurons])) # dimension must allow\n",
    "                                            # matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Operation Activation Function</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xW = tf.matmul(x, W) # It will do the dot multiplication, now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = tf.add(xW, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.sigmoid(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Variable Initializer!</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init) # do this for a session\n",
    "    \n",
    "    layer_out = sess.run(a, feed_dict={x:np.random.random([1, n_features])})\n",
    "                            # feeding it one sample\n",
    "    \n",
    "##endof:  with ... sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the result\n",
    "print(layer_out) \n",
    "      #  Mine doesn't match his Jose's, even though I used\n",
    "      #+ the same random seed and had the same results above\n",
    "      #+ ... (?)\n",
    "      #+ He says it might be different, depending on how many\n",
    "      #+ times we had run random. I guess he ran random a few\n",
    "      #+ times in between"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just passed in random numbers for `W` and `b`; we're not adjusting them.\n",
    "\n",
    "We still need to finish off this process with optimization! Let's learn how to do this next.\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Network Example\n",
    "\n",
    "Let's work on a regression example, we are trying to solve a very simple equation:\n",
    "\n",
    "y = mx + b\n",
    "\n",
    "y will be the y_labels and x is the x_data. We are trying to figure out the slope and the intercept for the line that best fits our data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial Data (Some Made Up Regression Data)\n",
    "\n",
    "a.k.a. (what the lecture calls it)\n",
    "\n",
    "### Simple Regression Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.linspace(0, 10, 10) + np.random.uniform(-1.5, 1.5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_label = np.linspace(0, 10, 10) + np.random.uniform(-1.5, 1.5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But that one matched (?). Maybe since it's np.random.uniform ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_data, y_label, '*') #  What have we created?\n",
    "                               #+ Hopefully something with\n",
    "                               #+ a linear trend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Variables</b>\n",
    "\n",
    "Remember, trying to solve $ y = mx + b $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.rand(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just copying them in - 0.44 and 0.88. Wait, he wants us to use something different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, b = np.random.rand(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"m = \" + str(m) + \" ; \" + \"b = \" + str(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimizer needs tensors. Otherwise, trying to run\n",
    "'optimizer.minimize(error)' below will give:\n",
    "\n",
    "```\n",
    "---------------------------------------------------------------------------\n",
    "AttributeError                            Traceback (most recent call last)\n",
    "<ipython-input-40-f5a4ebeb180e> in <module>()\n",
    "      1 optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "----> 2 train = optimizer.minimize(error)\n",
    "\n",
    "~\\.conda\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py in \\\\\n",
    "minimize(self, loss, global_step, var_list, gate_gradients, aggregation_method, \\\\\n",
    "         colocate_gradients_with_ops, name, grad_loss)\n",
    "    398         aggregation_method=aggregation_method,\n",
    "    399         colocate_gradients_with_ops=colocate_gradients_with_ops,\n",
    "--> 400         grad_loss=grad_loss)\n",
    "    401 \n",
    "    402     vars_with_grad = [v for g, v in grads_and_vars if g is not None]\n",
    "\n",
    "~\\.conda\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py in \\\\\n",
    "compute_gradients(self, loss, var_list, gate_gradients, aggregation_method, \\\\\n",
    "                  colocate_gradients_with_ops, grad_loss)\n",
    "    492                        \"Optimizer.GATE_OP, Optimizer.GATE_GRAPH.  Not %s\" %\n",
    "    493                        gate_gradients)\n",
    "--> 494     self._assert_valid_dtypes([loss])\n",
    "    495     if grad_loss is not None:\n",
    "    496       self._assert_valid_dtypes([grad_loss])\n",
    "\n",
    "~\\.conda\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py in \\\\\n",
    "_assert_valid_dtypes(self, tensors)\n",
    "    870     valid_dtypes = self._valid_dtypes()\n",
    "    871     for t in tensors:\n",
    "--> 872       dtype = t.dtype.base_dtype\n",
    "    873       if dtype not in valid_dtypes:\n",
    "    874         raise ValueError(\n",
    "\n",
    "AttributeError: 'numpy.dtype' object has no attribute 'base_dtype'\n",
    "```\n",
    "\n",
    "That's why I can't _just_ pull them out of the array, like I did. I'll cast them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = tf.Variable(m)\n",
    "b = tf.Variable(b)\n",
    "  # It's pythonic to repeat the variable, but I don't like it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip will do tuples with points\n",
    "\n",
    "for x, y in zip(x_data, y_label):\n",
    "    y_hat = m*x + b # predicted y, m and b are tries\n",
    "    \n",
    "    error += (y - y_hat) ** 2\n",
    "##endof:  for x, y ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "Learning rate: we don't want to overshoot the value, nor do we want to take forever to train things. Computations are expensive, and that matters with networks, because time matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "train = optimizer.minimize(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Session and Run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    training_steps = 1 # probably way too low\n",
    "    \n",
    "    for i in range(training_steps):\n",
    "        \n",
    "        sess.run(train)\n",
    "        \n",
    "    ##endof:  for i ...\n",
    "\n",
    "    final_slope, final_intercept = sess.run([m, b]) # expecting a very bad job\n",
    "    \n",
    "#endof:  with ... sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.linspace(-1, 11, 10) # Make plot look a little nicer\n",
    "\n",
    "# y = mx + b\n",
    "y_pred_plot = final_slope * x_test + final_intercept\n",
    "\n",
    "plt.plot(x_test, y_pred_plot, 'r')\n",
    "\n",
    "plt.plot(x_data, y_label, '*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It did decently with one epoch (one step). Let's go with 100.\n",
    "\n",
    "### Do it with 100 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess_100:\n",
    "    \n",
    "    sess_100.run(init)\n",
    "    \n",
    "    #training_steps = 1 # 100 will be better\n",
    "    epochs = 100\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        \n",
    "        sess_100.run(train)\n",
    "        \n",
    "    ##endof:  for i ...\n",
    "\n",
    "    # Fetch the results\n",
    "    final_slope_100, final_intercept_100 = sess_100.run([m, b])\n",
    "    \n",
    "#endof:  with ... sess_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_slope_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_intercept_100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate with 100 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.linspace(-1, 11, 10) # Make plot look a little nicer\n",
    "\n",
    "# y = mx + b\n",
    "y_pred_plot_100 = final_slope_100 * x_test + final_intercept_100\n",
    "\n",
    "plt.plot(x_test, y_pred_plot_100, 'r')\n",
    "\n",
    "plt.plot(x_data, y_label, '*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>That does indeed seem better</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_That's all for now!_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
