{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Regression Example\n",
    "\n",
    "More Realistic. More data points. Batches.\n",
    "\n",
    "The tf.estimator is for things that are easier. TensorFlow is more for things that need a specific neural network, customized, whatever..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# remember TensorFlow and SciKit-Learn up here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Million Points!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.linspace(0.0, 10.0, 1000000) # We're not quite ready for a real dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No random seed (?)\n",
    "noise = np.random.randn(len(x_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, for the data\n",
    "\n",
    "$ y = mx + b + noise $ just to make it more difficult for the model\n",
    "\n",
    "Jose, seemingly arbitrarily, chooses $ b = 5 $ and $ m = 0.5 $ to start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = ( 0.5 * x_data ) + 5 + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df = pd.DataFrame(data=x_data, columns=['X Data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df = pd.DataFrame(data=y_true, columns=['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = pd.concat(\n",
    "         [ x_df, y_df], axis=1) \n",
    "              # axis=1 keeps it from stacking on like pancakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copied from the course notes version:\n",
    "\n",
    "```\n",
    "my_data = pd.concat(\n",
    "        [pd.DataFrame(data=x_data,columns=['X Data']),\n",
    "         pd.DataFrame(data=y_true,columns=['Y'])\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_data.plot() might crash the kernel\n",
    "my_sample = my_data.sample(n=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sample.plot(kind='scatter', x = 'X Data', y='Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Size\n",
    "\n",
    "> We will take the data in batches (1 000 000 points is a lot to pass in at once)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random points to grab, If you had a trillion, probably use smaller batches\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_pre, b_pre = np.random.randn(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(m_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(b_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I didn't follow this, because using 'dtype' instead of 'type' worked\n",
    "#tf.cast(m_pre, tf.float32)\n",
    "#tf.cast(b_pre, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  DWB, I had to add the type\n",
    "#+ Jose just used, e.g. 'm = tf.Variable(0.81)'\n",
    "m = tf.Variable(m_pre, dtype=tf.float32) \n",
    "b = tf.Variable(b_pre, dtype=tf.float32)\n",
    "\n",
    "print(\"Initally: m = \" + str(m_pre) + \" ; \" + \"b = \" + str(b_pre))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ph = tf.placeholder(tf.float32, [batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ph = tf.placeholder(tf.float32, [batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, I'm getting that placeholders get your data, while variables are what you're trying to predict. I'm not sure that's exactly correct, but it's what I'm getting right now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph\n",
    " \n",
    "What are we trying to do here? Fit a line to some points. So it's a $ y = mx + b $ kind of graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_model = m * x_ph + b # Had to mess with type to get this to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Remember that y_value is the true value\n",
    "#+ Also, we square it to punish the error more,\n",
    "#+ and thus bring it closer more quickly.\n",
    "#+ could use '() ** 2' instead of tf.square()\n",
    "\n",
    "error = tf.reduce_sum(tf.square(y_ph - y_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "train = optimizer.minimize(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    batches = 1000\n",
    "    \n",
    "    for i in range(batches):\n",
    "        \n",
    "        rand_index = np.random.randint(len(x_data),\n",
    "                                       size=batch_size)\n",
    "        \n",
    "        #  DWB: it seems to me we'll only we doing 8000 out\n",
    "        #+ of the 1e6 points. That will make it go faster, I\n",
    "        #+ guess.\n",
    "        #\n",
    "        #  Jose says we can play around with batches and\n",
    "        #+ batch_size to see if we have enough data to \n",
    "        #+ train it well. He seems to suggest that, if we\n",
    "        #+ were to use more of the training data, we would\n",
    "        #+ overfit to the training data. Not sure if that\n",
    "        #+ applies here ... wait, yes it kinda does but not\n",
    "        #+ in a way that's too concerning - we're taking\n",
    "        #+ random parts ...\n",
    "        \n",
    "        feed = {x_ph:x_data[rand_index], \n",
    "                y_ph:y_true[rand_index]}\n",
    "                \n",
    "        sess.run(train, feed_dict=feed)\n",
    "        \n",
    "        #  So, we have it fitting the data with 8 random points\n",
    "        #+ for each\n",
    "        \n",
    "    ##endof:  for i\n",
    "    \n",
    "    #  Fetch the slope and intercept values (run will go get the \n",
    "    #+ m and b placeholders)\n",
    "    model_m, model_b = sess.run([m, b])\n",
    "    \n",
    "##endof:  with ... sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_m # should come out close to our 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b #should come out close to our 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we went from whatever our original m and b values were - in my case $ m = -1.8 $ and $ b = 0.5 $. The values used for this specific training can be found with the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"m_init = \" + str(m_pre) + \" ; \" + \"b_init = \" + str(b_pre))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we ended up with the `model_m` and `model_b` shown above, which are quite close to the values before noise, m = 0.5, b = 5; Things would look even nicer if we took the error over the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Delta_m = \" + str(abs(0.5 - model_m)) + \";\\n\" + \\\n",
    "      \"Delta_b = \" + str(abs(5.0 - model_b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = x_data * model_m + model_b # rem. y_hat represents the predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data.sample(250).plot(kind=\"scatter\",\n",
    "                         x=\"X Data\", y=\"Y\")\n",
    "plt.plot(x_data, y_hat, 'r') #  Oh, so I see Pandas is using \n",
    "                             #+ the matplotlib canvas. Cool!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jose changes the above code for 10k batches, I'm going to have it all re-written, so I can compare better. I will stick it to the anti-Q&R voice by not renaming the variables. Wahahaha!\n",
    "\n",
    "I though I might have to rename them, then I think I figured that I could get rid of an error that came up by initializing the variables. Nope, had to re-put-in all the code. But I'm not renaming the variables. Wahahaha!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  DWB, I had to add the type\n",
    "#+ Jose just used, e.g. 'm = tf.Variable(0.81)'\n",
    "m = tf.Variable(m_pre, dtype=tf.float32) \n",
    "b = tf.Variable(b_pre, dtype=tf.float32)\n",
    "print(\"Initally: m = \" + str(m_pre) + \" ; \" + \"b = \" + str(b_pre))\n",
    "x_ph = tf.placeholder(tf.float32, [batch_size])\n",
    "y_ph = tf.placeholder(tf.float32, [batch_size])\n",
    "y_model = m * x_ph + b\n",
    "error = tf.reduce_sum(tf.square(y_ph - y_model))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "train = optimizer.minimize(error)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    batches = 10000\n",
    "    \n",
    "    for i in range(batches):\n",
    "        \n",
    "        rand_index = np.random.randint(len(x_data),\n",
    "                                       size=batch_size)\n",
    "        \n",
    "        feed = {x_ph:x_data[rand_index], \n",
    "                y_ph:y_true[rand_index]}\n",
    "                \n",
    "        sess.run(train, feed_dict=feed)\n",
    "        \n",
    "    ##endof:  for i\n",
    "    \n",
    "    #  Fetch the slope and intercept values (run will go get the \n",
    "    #+ m and b placeholders)\n",
    "    model_m, model_b = sess.run([m, b])\n",
    "    \n",
    "##endof:  with ... sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After re-puttting-in the code, I got my answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"m_init = \" + str(m_pre) + \" ; \" + \"b_init = \" + str(b_pre))\n",
    "print(\"m_final = \" + str(model_m) + \" ; \" + \"b_fin = \" + str(model_b))\n",
    "\n",
    "print(\"Delta_m = \" + str(abs(0.5 - model_m)) + \";\\n\" + \\\n",
    "      \"Delta_b = \" + str(abs(5.0 - model_b)))\n",
    "print()\n",
    "print(\"Hmmm ...\")\n",
    "print()\n",
    "print(\"Compare to:\" + '\\n' + \\\n",
    "      \"Delta_m = 0.006303846836090088\" + '\\n' + \"and\" + \\\n",
    "      '\\n' + \"Delta_b = 0.055045127868652344\" + '\\n' +  \\\n",
    "      \"for 8000 batches.\" + \\\n",
    "      '\\n\\n' + \"... interesting ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = x_data * model_m + model_b\n",
    "my_data.sample(250).plot(kind=\"scatter\",\n",
    "                         x=\"X Data\", y=\"Y\")\n",
    "plt.plot(x_data, y_hat, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jose stated that the noise might make it so they might not be so different.\n",
    "\n",
    "He noted (as I'd been thinking) that we haven't been doing the train/test split. We will with `tf.estimator`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.estimator API\n",
    "\n",
    "> Much simpler API for basic tasks like regression! We'll talk about more abstractions like TF-Slim later on.\n",
    "\n",
    "### Types\n",
    "\n",
    "```\n",
    "tf.estimator.LinearClassifier\n",
    "tf.estimator.LinearRegressor\n",
    "tf.estimator.DNNClassifier\n",
    "tf.estimator.DNNRegressor\n",
    "```\n",
    "\n",
    "Jose says `DNN` is for Densely-connected Neural Network. I'm not sure that it's not Deep, but I am leaning towards thinking he's right.\n",
    "\n",
    "Combined-type\n",
    "\n",
    "```\n",
    "tf.estimator.DNNLinearCombinedClassifier\n",
    "tf.estimator.DNNLinearCombinedRegressor\n",
    "```\n",
    "\n",
    "### Steps\n",
    "\n",
    "- Define a list of feature columns\n",
    "- Create the Estimator Model\n",
    "- Create a Data Input Function\n",
    "- Call `train`, `evaluate`, and `predict` on the object\n",
    "\n",
    "#### We're going to use maybe-not-the-best use case, with just one feature, but it will get us the idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deeper dive into feature column later\n",
    "feat_cols = [ tf.feature_column.numeric_column('x', shape=[1]) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = tf.estimator.LinearRegressor(feature_columns=feat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the output\n",
    "\n",
    "```\n",
    "INFO:tensorflow:Using default config.\n",
    "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\Anast\\AppData\\Local\\Temp\\tmpn9_bnvpm\n",
    "INFO:tensorflow:Using config: {'_session_config': None, '_device_fn': None, '_keep_checkpoint_max': 5, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002E2998E5128>, '_task_type': 'worker', '_global_id_in_cluster': 0, '_num_worker_replicas': 1, '_log_step_count_steps': 100, '_is_chief': True, '_train_distribute': None, '_save_checkpoints_secs': 600, '_task_id': 0, '_service': None, '_save_summary_steps': 100, '_keep_checkpoint_every_n_hours': 10000, '_num_ps_replicas': 0, '_master': '', '_tf_random_seed': None, '_evaluation_master': '', '_save_checkpoints_steps': None, '_model_dir': 'C:\\\\Users\\\\Anast\\\\AppData\\\\Local\\\\Temp\\\\tmpn9_bnvpm'}\n",
    "```\n",
    "\n",
    "with probably a different string at the end of the path each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split\n",
    "\n",
    "> We haven't actually performed a train test split yet! So let's do that on our data now and perform a more realistic version of a Regression Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type 'train_test_split', then do [Shift] + [Tab] to get the line\n",
    "x_train, x_eval, y_train, y_eval = \\\n",
    "                train_test_split(x_data, y_true, \n",
    "                                 test_size=0.3, \n",
    "                                 random_state=101) # match Jose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"x_train.shape = \" + str(x_train.shape))\n",
    "print(\"y_train.shape = \" + str(y_train.shape))\n",
    "print()\n",
    "print(\"x_eval.shape = \" + str(x_eval.shape))\n",
    "print(\"y_eval.shape = \" + str(y_eval.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Estimator Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ... input_function that kind of serves like your feed dictionary and your batch size indicator ...\n",
    "\n",
    "Jose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can also do .pandas_input_fn if you're coming from a Pandas dataframe\n",
    "input_func = \\\n",
    "    tf.estimator.inputs.numpy_input_fn({'x':x_train},\n",
    "                                       y_train,\n",
    "                                       batch_size=4,\n",
    "                                       num_epochs=None,\n",
    "                                       shuffle=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_func = \\\n",
    "    tf.estimator.inputs.numpy_input_fn({'x':x_train},\n",
    "                                       y_train,\n",
    "                                       batch_size=4,\n",
    "                                       num_epochs=1000,\n",
    "                                       shuffle=False\n",
    "    )\n",
    "#  Why shuffle is False?\n",
    "#+ Jose says, \"And I also wanna set shuffle equal to false.\n",
    "#+            \"Okay, there we go.\n",
    "#+            \"And the reason I have a shuffle equals false \n",
    "#+            \"here for this\n",
    "#+            \"train is because I'm gonna be using this \n",
    "#+            \"train input\n",
    "#+            \"function for evaluation against a test input \n",
    "#+            \"function.\"\n",
    "#+ DWB:  The one just below, eval_input_function, which also\n",
    "#+ DWB: has shuffle=False\n",
    "#+ DWB:  Not sure I understand this, but let's go with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_input_func = \\\n",
    "    tf.estimator.inputs.numpy_input_fn({'x':x_eval},\n",
    "                                       y_eval,\n",
    "                                       batch_size=4,\n",
    "                                       num_epochs=1000,\n",
    "                                       shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.train(input_fn=input_func, steps=1000)\n",
    "    #  We do  steps=1000 , since we didn't specify training\n",
    "    #+ epochs for our  input_func ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics = estimator.evaluate(input_fn=train_input_func,\n",
    "                                   steps=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics = estimator.evaluate(input_fn=eval_input_func,\n",
    "                                  steps=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train metrics: {}\".format(train_metrics)) # Python 3.5 allows this form\n",
    "print(\"eval metrics : {}\".format(eval_metrics))\n",
    "print(\"The loss is pretty close for both, which \" + \\\n",
    "      \"\\nis a good sign that we're not overfitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_new_data = np.linspace(0, 10, 10)\n",
    "input_fn_predict = \\\n",
    "    tf.estimator.inputs.numpy_input_fn({'x':brand_new_data},\n",
    "                                       shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.predict(input_fn=input_fn_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easy to see the output of the generator by casting it as a list\n",
    "list(estimator.predict(input_fn=input_fn_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat, for size\n",
    "my_list = list(estimator.predict(input_fn=input_fn_predict))\n",
    "print(\"\\nThere are \" + str(len(my_list)) + \" entries,\" + \\\n",
    "      \"\\nas we expect (as long as there are 10 entries).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [] #could also use  np.array[]\n",
    "for x in estimator.predict(input_fn=input_fn_predict):\n",
    "    predictions.append(x['predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions # how did we do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data.sample(n=250).plot(kind='scatter',\n",
    "                           x='X Data', y='Y')\n",
    "plt.plot(brand_new_data, predictions, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data.sample(n=250).plot(kind='scatter',\n",
    "                           x='X Data', y='Y')\n",
    "plt.plot(brand_new_data, predictions, 'r*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jose usually says\n",
    "\n",
    "# Great Job!\n",
    "\n",
    "around here, which I think is cool.\n",
    "\n",
    "_That's all for now!_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
