{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Regression Example\n",
    "\n",
    "More Realistic. More data points. Batches.\n",
    "\n",
    "The tf.estimator is for things that are easier. TensorFlow is more for things that need a specific neural network, customized, whatever..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# remember TensorFlow and SciKit-Learn up here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Million Points!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.linspace(0.0, 10.0, 1000000) # We're not quite ready for a real dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No random seed (?)\n",
    "noise = np.random.randn(len(x_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, for the data\n",
    "\n",
    "$ y = mx + b + noise $ just to make it more difficult for the model\n",
    "\n",
    "Jose, seemingly arbitrarily, chooses $ b = 5 $ and $ m = 0.5 $ to start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = ( 0.5 * x_data ) + 5 + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df = pd.DataFrame(data=x_data, columns=['X Data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df = pd.DataFrame(data=y_true, columns=['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = pd.concat(\n",
    "         [ x_df, y_df], axis=1) \n",
    "              # axis=1 keeps it from stacking on like pancakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copied from the course notes version:\n",
    "\n",
    "```\n",
    "my_data = pd.concat(\n",
    "        [pd.DataFrame(data=x_data,columns=['X Data']),\n",
    "         pd.DataFrame(data=y_true,columns=['Y'])\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_data.plot() might crash the kernel\n",
    "my_sample = my_data.sample(n=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sample.plot(kind='scatter', x = 'X Data', y='Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Size\n",
    "\n",
    "> We will take the data in batches (1 000 000 points is a lot to pass in at once)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random points to grab, If you had a trillion, probably use smaller batches\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_pre, b_pre = np.random.randn(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(m_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(b_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I didn't follow this, because using 'dtype' instead of 'type' worked\n",
    "#tf.cast(m_pre, tf.float32)\n",
    "#tf.cast(b_pre, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  DWB, I had to add the type\n",
    "#+ Jose just used, e.g. 'm = tf.Variable(0.81)'\n",
    "m = tf.Variable(m_pre, dtype=tf.float32) \n",
    "b = tf.Variable(b_pre, dtype=tf.float32)\n",
    "\n",
    "print(\"Initally: m = \" + str(m_pre) + \" ; \" + \"b = \" + str(b_pre))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ph = tf.placeholder(tf.float32, [batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ph = tf.placeholder(tf.float32, [batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, I'm getting that placeholders get your data, while variables are what you're trying to predict. I'm not sure that's exactly correct, but it's what I'm getting right now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph\n",
    " \n",
    "What are we trying to do here? Fit a line to some points. So it's a $ y = mx + b $ kind of graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_model = m * x_ph + b # Had to mess with type to get this to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Remember that y_value is the true value\n",
    "#+ Also, we square it to punish the error more,\n",
    "#+ and thus bring it closer more quickly.\n",
    "#+ could use '() ** 2' instead of tf.square()\n",
    "\n",
    "error = tf.reduce_sum(tf.square(y_ph - y_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "train = optimizer.minimize(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    batches = 1000\n",
    "    \n",
    "    for i in range(batches):\n",
    "        \n",
    "        rand_index = np.random.randint(len(x_data),\n",
    "                                       size=batch_size)\n",
    "        \n",
    "        #  DWB: it seems to me we'll only we doing 8000 out\n",
    "        #+ of the 1e6 points. That will make it go faster, I\n",
    "        #+ guess.\n",
    "        #\n",
    "        #  Jose says we can play around with batches and\n",
    "        #+ batch_size to see if we have enough data to \n",
    "        #+ train it well. He seems to suggest that, if we\n",
    "        #+ were to use more of the training data, we would\n",
    "        #+ overfit to the training data. Not sure if that\n",
    "        #+ applies here ... wait, yes it kinda does but not\n",
    "        #+ in a way that's too concerning - we're taking\n",
    "        #+ random parts ...\n",
    "        \n",
    "        feed = {x_ph:x_data[rand_index], \n",
    "                y_ph:y_true[rand_index]}\n",
    "                \n",
    "        sess.run(train, feed_dict=feed)\n",
    "        \n",
    "        #  So, we have it fitting the data with 8 random points\n",
    "        #+ for each\n",
    "        \n",
    "    ##endof:  for i\n",
    "    \n",
    "    #  Fetch the slope and intercept values (run will go get the \n",
    "    #+ m and b placeholders)\n",
    "    model_m, model_b = sess.run([m, b])\n",
    "    \n",
    "##endof:  with ... sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_m # should come out close to our 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b #should come out close to our 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we went from whatever our original m and b values were - in my case $ m = -1.8 $ and $ b = 0.5 $. The values used for this specific training can be found with the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"m_init = \" + str(m_pre) + \" ; \" + \"b_init = \" + str(b_pre))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we ended up with the `model_m` and `model_b` shown above, which are quite close to the values before noise, m = 0.5, b = 5; Things would look even nicer if we took the error over the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Delta_m = \" + str(abs(0.5 - model_m)) + \";\\n\" + \\\n",
    "      \"Delta_b = \" + str(abs(5.0 - model_b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = x_data * model_m + model_b # rem. y_hat represents the predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data.sample(250).plot(kind=\"scatter\",\n",
    "                         x=\"X Data\", y=\"Y\")\n",
    "plt.plot(x_data, y_hat, 'r') #  Oh, so I see Pandas is using \n",
    "                             #+ the matplotlib canvas. Cool!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jose changes the above code for 10k batches, I'm going to have it all re-written, so I can compare better. I will stick it to the anti-Q&R voice by not renaming the variables. Wahahaha!\n",
    "\n",
    "I though I might have to rename them, then I think I figured that I could get rid of an error that came up by initializing the variables. Nope, had to re-put-in all the code. But I'm not renaming the variables. Wahahaha!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  DWB, I had to add the type\n",
    "#+ Jose just used, e.g. 'm = tf.Variable(0.81)'\n",
    "m = tf.Variable(m_pre, dtype=tf.float32) \n",
    "b = tf.Variable(b_pre, dtype=tf.float32)\n",
    "print(\"Initally: m = \" + str(m_pre) + \" ; \" + \"b = \" + str(b_pre))\n",
    "x_ph = tf.placeholder(tf.float32, [batch_size])\n",
    "y_ph = tf.placeholder(tf.float32, [batch_size])\n",
    "y_model = m * x_ph + b\n",
    "error = tf.reduce_sum(tf.square(y_ph - y_model))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "train = optimizer.minimize(error)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    batches = 10000\n",
    "    \n",
    "    for i in range(batches):\n",
    "        \n",
    "        rand_index = np.random.randint(len(x_data),\n",
    "                                       size=batch_size)\n",
    "        \n",
    "        feed = {x_ph:x_data[rand_index], \n",
    "                y_ph:y_true[rand_index]}\n",
    "                \n",
    "        sess.run(train, feed_dict=feed)\n",
    "        \n",
    "    ##endof:  for i\n",
    "    \n",
    "    #  Fetch the slope and intercept values (run will go get the \n",
    "    #+ m and b placeholders)\n",
    "    model_m, model_b = sess.run([m, b])\n",
    "    \n",
    "##endof:  with ... sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After re-puttting-in the code, I got my answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"m_init = \" + str(m_pre) + \" ; \" + \"b_init = \" + str(b_pre))\n",
    "print(\"m_final = \" + str(model_m) + \" ; \" + \"b_fin = \" + str(model_b))\n",
    "\n",
    "print(\"Delta_m = \" + str(abs(0.5 - model_m)) + \";\\n\" + \\\n",
    "      \"Delta_b = \" + str(abs(5.0 - model_b)))\n",
    "print()\n",
    "print(\"Hmmm ...\")\n",
    "print()\n",
    "print(\"Compare to:\" + '\\n' + \\\n",
    "      \"Delta_m = 0.006303846836090088\" + '\\n' + \"and\" + \\\n",
    "      '\\n' + \"Delta_b = 0.055045127868652344\" + '\\n' +  \\\n",
    "      \"for 8000 batches.\" + \\\n",
    "      '\\n\\n' + \"... interesting ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = x_data * model_m + model_b\n",
    "my_data.sample(250).plot(kind=\"scatter\",\n",
    "                         x=\"X Data\", y=\"Y\")\n",
    "plt.plot(x_data, y_hat, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jose stated that the noise might make it so they might not be so different.\n",
    "\n",
    "He noted (as I'd been thinking) that we haven't been doing the train/test split. We will with `tf.estimator`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.estimator API\n",
    "\n",
    "> Much simpler API for basic tasks like regression! We'll talk about more abstractions like TF-Slim later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split\n",
    "\n",
    "> We haven't actually performed a train test split yet! So let's do that on our data now and perform a more realistic version of a Regression Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Estimator Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_That's all for now!_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
